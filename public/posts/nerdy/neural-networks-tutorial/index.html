<!DOCTYPE html>
<head>
  <title>no mad blog</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" type="image/png" href="/imgs/icons/favicon.png">
  <link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.teal-deep_orange.min.css" />
  <script defer src="https://code.getmdl.io/1.3.0/material.min.js"></script>
  <link rel="stylesheet" href="/css/article.css" type="text/css">
  <link rel="stylesheet" href="/css/github.css" type="text/css">
  
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha384-dq1/gEHSxPZQ7DdrM82ID4YVol9BYyU7GbWlIwnwyPzotpoc57wDw/guX8EaYGPx" crossorigin="anonymous"></script>
  
  <script data-isso="//comments.nomadblog.de/"
        src="//comments.nomadblog.de/js/embed.min.js"></script>
</head>

<body>
  <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header">
    <header class="mdl-layout__header">
    <div class="mdl-layout__header-row">
      <a href="#" onclick="history.go(-1)">
        <button class="mdl-button mdl-js-button">
          <i class="material-icons">arrow_back</i>
        </button></a>
        <a href=https://nomadblog.de/>
          <span class="mdl-layout-title">no mad blog</span>
        </a>
        <i class="mdl-cell--hide-phone material-icons">keyboard_arrow_right</i>
        <a href="/posts" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-cell--hide-phone">posts</a>
        <i class="mdl-cell--hide-phone material-icons">keyboard_arrow_right</i>
        <button class="mdl-button mdl-js-button">
          <a class="mdl-cell--hide-phone"> Neural Networks Tutorial</a>
        </button>
    </div>
</header>
    <main class="mdl-layout__content">
      <div class="page-content">
        <article class="container--content">
          <div class="mdl-grid mdl-grid--no-spacing">
            
            <div class="image logo mdl-cell mdl-cell--12-col" style="background-image: url(/imgs/logos/nerdy/neural_networks_tutorial2.png)"></div>
            
            <div class="mdl-cell mdl-cell--12-col">
              
              <h3 class="title text--centered">Neural Networks Tutorial</h3>
              
            </div>
          </div>
          <div class="mdl-grid mdl-grid">
            <div class="mdl-layout-spacer"></div>
            <div class="content mdl-cell mdl-cell--5-col">
              

<p>Artificial Neural Networks (ANNs) are <em>the</em> hype in Computer Science, and I have jumped on the bandwagon.
I will surely write about how I want to apply ANNs to the field of communication networks in some later post, but for now I want to present what I have gathered about these lovely little thingies, and show it in a way that would have been very useful to me when I first started looking into ANNs:</p>

<p><strong>I will showcase a brief, hands-on example of how ANNs work, and provide some TensorFlow / Keras code to back up my claims.</strong></p>

<p>The examples are based on <a href="https://www.coursera.org/learn/machine-learning/home/welcome"><strong>Andrew Ng&rsquo;s course on Machine Learning on coursera.org</strong></a> - I can certainly recommend this course. The Python code is written by me, and uses <code>tensorflor v1.14</code> and its built-in Keras library.</p>

<p>To keep things short, I won&rsquo;t introduce everything about ANNs; you can easily find info on this on thousands of other sites, online courses and books.
Instead, the practical example of <em>learning logic operators through ANNs</em> from the above-mentioned online course is given.</p>

<h1 id="learning-the-logical-and">Learning the logical <code>AND</code></h1>

<p>You probably know the <code>AND</code> operator. Its truth table fully defines its function (output 1 only if both inputs are 1):</p>

<table class="mdl-data-table mdl-js-data-table mdl-shadow--2dp">
  <thead>
    <tr>
      <th>x1</th>
      <th>x2</th>
      <th>x1 AND x2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>We can approximate the <code>AND</code> function with a single-layer Neural Network. Learning the truth table above for a number of epochs, it would learn the following weights:
<img src="/imgs/neural-network-tutorial/ann_AND.png" alt="Neural Network" /></p>

<p>and by setting the activation function as the <em>Sigmoid function</em>
$$ g(z) = \frac{1}{1 + e^{-\Theta^T z}} $$
the ANN is capable of learning the <code>AND</code> function, as its prediction becomes
$$ h_\theta(x) = g(-30 + 20x_1 + 20x_2) $$
which leads to</p>

<table class="mdl-data-table mdl-js-data-table mdl-shadow--2dp">
  <thead>
    <tr>
      <th>x1</th>
      <th>x2</th>
      <th>x1 AND x2</th>
      <th>prediction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>g(-30) ≈ 0</td>
    </tr>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>g(-10) ≈ 0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>g(-10) ≈ 0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>g(10) ≈ 1</td>
    </tr>
  </tbody>
</table>

<h2 id="and-now-in-python">And now in Python</h2>

<pre><code>import tensorflow.compat.v1 as tf
from tensorflow.keras import layers
import numpy as np

def construct_single_layer_ann(learning_rate, num_inputs, num_outputs):
	model = tf.keras.Sequential()	
	model.add(layers.Dense(units=num_outputs, activation='sigmoid', input_shape=(num_inputs,)))
	# Gradient Descent + Mean Squared Error loss function.
	model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate), loss='mse', metrics=['accuracy'])
	return model

def get_training_data_AND():
	data = [[(0,0), (0,1), (1,0), (1,1)]]
	labels = [0, 0, 0, 1]
	return data, labels

def print_layer_weights(model, num_layer):
	layer_weights = model.layers[num_layer].get_weights()[0]
	bias_weights = model.layers[num_layer].get_weights()[1]	
	print(&quot;Learned weights are&quot;, end=' ')	
	for i in range(len(layer_weights)):
		for j in range(len(layer_weights[i])):
			print(&quot;n_&quot; + str(num_layer+1) + &quot;,&quot; + str(i+1) + &quot;_&quot; + str(j) + &quot;=&quot; + str(layer_weights[i][0]), end=' ')
	for i in range(len(bias_weights)):
		print(&quot;b_&quot; + str(num_layer+1) + &quot;,&quot; + str(i) + &quot;=&quot; + str(bias_weights[i]), end=' ')
	print()

if __name__ == &quot;__main__&quot;:
	learning_rate = 10.0
	num_epochs = 50
	verbose = False

	# Logical AND model.	
	model_AND = construct_single_layer_ann(learning_rate, 2, 1)
	# Provide perfect training data.
	data_AND, labels_AND = get_training_data_AND()
	# Fit the model to the data.
	history = LossHistory()
	model_AND.fit(data_AND, labels_AND, epochs=num_epochs, verbose=verbose, callbacks=[history])
	# Print weights and loss.
	print(&quot;Logical AND ANN:\n================&quot;)
	print_layer_weights(model_AND, 0)
	print(&quot;Final loss is &quot; + str(history.losses[-1]))	
	predictions = model_AND.predict(data_AND)
	for i in range(predictions.size):		
		print(str(data_AND[0][i]) + &quot; -&gt; &quot; + str(predictions[i][0]) + &quot; ≈ &quot; + str(round(predictions[i][0])))	
	print()
</code></pre>

<p>which outputs</p>

<pre><code>Logical AND ANN:
================
Learned weights are n_1,1_0=3.8320987 n_1,2_0=3.8320732 b_1,0=-5.8527803 
Final loss is 0.012041996
(0, 0) -&gt; 0.002863679 ≈ 0.0
(0, 1) -&gt; 0.117045894 ≈ 0.0
(1, 0) -&gt; 0.11704853 ≈ 0.0
(1, 1) -&gt; 0.85953003 ≈ 1.0
</code></pre>

<p>We can see that instead of learning the weights -30, 20, 20 it learns -5.8, 3.8, 3.8. The <em>relations</em> are the same though
$$\frac{-30}{20} = -1.5 \approx{} \frac{-5.8}{3.8} = -1.53$$</p>

<p>This concludes this very brief introduction. The practical example of the <code>AND</code> function seemed intuitive to me. All an ANN does is adapt its weights until the loss (prediction - target) is adequately small. Since the Sigmoid function is non-linear, the ANN can approximate non-linear functions. And Keras gives us a way of implementing this in very few lines of code. :)</p>

            </div>
            <div class="mdl-layout-spacer"></div>
          </div>
        </article>
        <hr/>
<div class="mdl-grid mdl-grid--no-spacing">    
    <div class="mdl-cell mdl-cell--12-col">
        <h4 class="text--centered">Comments</h4>
    </div>    
</div>
<div class="mdl-grid">    
    <div class="mdl-layout-spacer"></div>
    <div class="mdl-cell mdl-cell--5-col">
        <section id="isso-thread"></section>
    </div>
    <div class="mdl-layout-spacer"></div>
</div>
        <footer class="mdl-mini-footer">
  <div class="mdl-mini-footer__left-section">
    <div class="mdl-logo">Contact</div>
    <ul class="mdl-mini-footer__link-list">
      <li><a href="mailto:sebastian@nomadblog.de" title="Send me a mail!" target="_self" class="mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect mdl-button--accent"><i class="material-icons">email</i></a></li>
    </ul>
  </div>
</footer>

      </div>
    </main>
  </div>
</body>

  <script>renderMathInElement(document.body);</script>


